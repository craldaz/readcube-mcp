{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf336936",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Demo of the LLM Label Retrieval](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22fb960",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Demo of the LLM Label Retrieval](#toc1_)    \n",
    "  - [Imports](#toc1_1_)    \n",
    "  - [BooleanQuery](#toc1_2_)    \n",
    "  - [DSPy Signatures](#toc1_3_)    \n",
    "  - [QueryToLabelsTranslator Module](#toc1_4_)    \n",
    "  - [Paper Filter Module](#toc1_5_)    \n",
    "  - [AdvancedQueryTranslator](#toc1_6_)    \n",
    "  - [Testing the AdvancedQueryTranslator](#toc1_7_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc130edf",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Imports](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ff5bd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from typing import List, Dict, Set, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84305753",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[BooleanQuery](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "badc8a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BooleanQuery:\n",
    "    \"\"\"Represents a boolean query over labels\"\"\"\n",
    "    must_have: Set[str]      # AND conditions\n",
    "    should_have: Set[str]    # OR conditions\n",
    "    must_not_have: Set[str]  # NOT conditions\n",
    "    confidence: float = 1.0\n",
    "\n",
    "    def __str__(self):\n",
    "        parts = []\n",
    "        if self.must_have:\n",
    "            parts.append(f\"MUST: {list(self.must_have)}\")\n",
    "        if self.should_have:\n",
    "            parts.append(f\"SHOULD: {list(self.should_have)}\")\n",
    "        if self.must_not_have:\n",
    "            parts.append(f\"NOT: {list(self.must_not_have)}\")\n",
    "        return \" | \".join(parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d8b4a2",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[DSPy Signatures](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "991dd8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryParser(dspy.Signature):\n",
    "    \"\"\"Parse natural language query into structured components using the available labels. Think broadly about the different clusters of labels.\"\"\"\n",
    "\n",
    "    query = dspy.InputField(desc=\"Natural language query from user\")\n",
    "    available_labels = dspy.InputField(\n",
    "        desc=\"List of available labels in the database which will be used to match concepts in the query to actual labels (comma-separated)\")\n",
    "    \n",
    "    main_concepts = dspy.OutputField(\n",
    "        desc=\"Primary concepts the user is looking for (comma-separated)\")\n",
    "    required_concepts = dspy.OutputField(\n",
    "        desc=\"Concepts that MUST be present (comma-separated, empty if none)\")\n",
    "    optional_concepts = dspy.OutputField(\n",
    "        desc=\"Concepts that SHOULD be present but not required (comma-separated, empty if none)\")\n",
    "    excluded_concepts = dspy.OutputField(\n",
    "        desc=\"Concepts that must NOT be present (comma-separated, empty if none)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5f0b0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelMatcher(dspy.Signature):\n",
    "    \"\"\"Match parsed concepts to actual database labels\"\"\"\n",
    "\n",
    "    concept = dspy.InputField(desc=\"A concept extracted from user query\")\n",
    "    available_labels = dspy.InputField(desc=\"List of available labels with their usage counts\")\n",
    "\n",
    "    matched_labels = dspy.OutputField(\n",
    "        desc=\"Best matching labels for this concept (comma-separated). MUST be from available_labels list only. Prefer labels with higher usage counts when multiple good matches exist.\")\n",
    "    confidence = dspy.OutputField(desc=\"Confidence score 0-1 for the matches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1966fd2",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[QueryToLabelsTranslator Module](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11aabcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryToLabelsTranslator(dspy.Module):\n",
    "    \"\"\"Main translator from natural language to boolean label queries with DSPy Refine validation\"\"\"\n",
    "\n",
    "    def __init__(self, available_labels: List[str], label_counts: Dict[str, int], max_retries: int = 3):\n",
    "        super().__init__()\n",
    "        self.available_labels = available_labels\n",
    "        self.label_set = set(available_labels)\n",
    "        self.label_counts = label_counts\n",
    "        self.max_retries = max_retries\n",
    "\n",
    "        # Create formatted label list with counts for LLM context\n",
    "        self.labels_with_counts = self._format_labels_with_counts()\n",
    "\n",
    "        # DSPy modules\n",
    "        self.query_parser = dspy.ChainOfThought(QueryParser)\n",
    "        \n",
    "        # Create reward function for label validation (correct signature: args, pred)\n",
    "        def label_validation_reward(args, pred: dspy.Prediction) -> float:\n",
    "            \"\"\"Reward function that returns 1.0 if all labels are valid, 0.0 otherwise\"\"\"\n",
    "            try:\n",
    "                matched_labels = []\n",
    "                if hasattr(pred, 'matched_labels') and pred.matched_labels.strip():\n",
    "                    matched_labels = [label.strip() for label in pred.matched_labels.split(',') if label.strip()]\n",
    "                \n",
    "                # Check if all labels are in available_labels\n",
    "                valid_labels = all(label in self.label_set for label in matched_labels)\n",
    "                return 1.0 if valid_labels else 0.0\n",
    "            except:\n",
    "                return 0.0\n",
    "        \n",
    "        # Wrap LabelMatcher with Refine for automatic feedback-based retries\n",
    "        self.label_matcher = dspy.Refine(\n",
    "            dspy.ChainOfThought(LabelMatcher),\n",
    "            N=max_retries,\n",
    "            reward_fn=label_validation_reward,\n",
    "            threshold=1.0  # Stop when we get perfect validation (reward = 1.0)\n",
    "        )\n",
    "\n",
    "    def _format_labels_with_counts(self) -> str:\n",
    "        \"\"\"Format labels with their counts for LLM context\"\"\"\n",
    "        # Sort labels by count (descending) to show most popular first\n",
    "        sorted_labels = sorted(self.label_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Format as \"label (count=X)\" \n",
    "        formatted_labels = [f\"{label} (count={count})\" for label, count in sorted_labels]\n",
    "        return \", \".join(formatted_labels)\n",
    "\n",
    "    def forward(self, query: str) -> BooleanQuery:\n",
    "        \"\"\"Translate natural language query to boolean label query\"\"\"\n",
    "\n",
    "        # Step 1: Parse the natural language query\n",
    "        parsed = self.query_parser(\n",
    "            query=query,\n",
    "            available_labels=self.labels_with_counts\n",
    "        )\n",
    "\n",
    "        # Step 2: Match concepts to actual labels\n",
    "        boolean_query = BooleanQuery(\n",
    "            must_have=set(),\n",
    "            should_have=set(),\n",
    "            must_not_have=set()\n",
    "        )\n",
    "\n",
    "        # Process main concepts (these become SHOULD conditions)\n",
    "        if parsed.main_concepts.strip():\n",
    "            main_labels = self._match_concepts_to_labels(\n",
    "                parsed.main_concepts.split(',')\n",
    "            )\n",
    "            boolean_query.should_have.update(main_labels)\n",
    "\n",
    "        # Process required concepts (these become MUST conditions)\n",
    "        if parsed.required_concepts.strip():\n",
    "            required_labels = self._match_concepts_to_labels(\n",
    "                parsed.required_concepts.split(',')\n",
    "            )\n",
    "            boolean_query.must_have.update(required_labels)\n",
    "\n",
    "        # Process optional concepts (these become additional SHOULD conditions)\n",
    "        if parsed.optional_concepts.strip():\n",
    "            optional_labels = self._match_concepts_to_labels(\n",
    "                parsed.optional_concepts.split(',')\n",
    "            )\n",
    "            boolean_query.should_have.update(optional_labels)\n",
    "\n",
    "        # Process excluded concepts (these become NOT conditions)\n",
    "        if parsed.excluded_concepts.strip():\n",
    "            excluded_labels = self._match_concepts_to_labels(\n",
    "                parsed.excluded_concepts.split(',')\n",
    "            )\n",
    "            boolean_query.must_not_have.update(excluded_labels)\n",
    "\n",
    "        return boolean_query\n",
    "\n",
    "    def _match_concepts_to_labels(self, concepts: List[str]) -> Set[str]:\n",
    "        \"\"\"Match a list of concepts to actual database labels using Refine validation\"\"\"\n",
    "        matched_labels = set()\n",
    "\n",
    "        for concept in concepts:\n",
    "            concept = concept.strip()\n",
    "            if not concept:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Use Refine wrapped LabelMatcher - it will automatically provide feedback and retry\n",
    "                matches = self.label_matcher(\n",
    "                    concept=concept,\n",
    "                    available_labels=self.labels_with_counts\n",
    "                )\n",
    "\n",
    "                # Parse matched labels - Refine ensures these are valid through feedback loop\n",
    "                if matches.matched_labels.strip():\n",
    "                    for label in matches.matched_labels.split(','):\n",
    "                        label = label.strip()\n",
    "                        if label and label in self.label_set:  # Double-check validity\n",
    "                            matched_labels.add(label)\n",
    "\n",
    "            except Exception as e:\n",
    "                # Handle any errors (network, JSON parsing, etc.)\n",
    "                print(f\"Warning: Could not match concept '{concept}' to valid labels: {e}\")\n",
    "                continue\n",
    "\n",
    "        return matched_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462f512d",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[Paper Filter Module](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfc68ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperFilter:\n",
    "    \"\"\"Filter papers based on boolean label queries\"\"\"\n",
    "\n",
    "    def __init__(self, papers_db: List[Dict]):\n",
    "        \"\"\"\n",
    "        papers_db: List of dicts with keys: 'id', 'title', 'abstract', 'labels'\n",
    "        \"\"\"\n",
    "        self.papers_db = papers_db\n",
    "\n",
    "    def filter_papers(self, boolean_query: BooleanQuery, min_should_match: int = 1) -> List[Dict]:\n",
    "        \"\"\"Filter papers based on boolean query\"\"\"\n",
    "\n",
    "        filtered_papers = []\n",
    "\n",
    "        for paper in self.papers_db:\n",
    "            paper_labels = set(paper.get('labels', []))\n",
    "\n",
    "            # Check MUST conditions\n",
    "            if boolean_query.must_have:\n",
    "                if not boolean_query.must_have.issubset(paper_labels):\n",
    "                    continue\n",
    "\n",
    "            # Check NOT conditions\n",
    "            if boolean_query.must_not_have:\n",
    "                if boolean_query.must_not_have.intersection(paper_labels):\n",
    "                    continue\n",
    "\n",
    "            # Check SHOULD conditions\n",
    "            if boolean_query.should_have:\n",
    "                should_matches = len(\n",
    "                    boolean_query.should_have.intersection(paper_labels))\n",
    "                if should_matches < min_should_match:\n",
    "                    continue\n",
    "\n",
    "                # Add relevance score based on how many SHOULD conditions match\n",
    "                paper['relevance_score'] = should_matches / \\\n",
    "                    len(boolean_query.should_have)\n",
    "            else:\n",
    "                paper['relevance_score'] = 1.0\n",
    "\n",
    "            filtered_papers.append(paper)\n",
    "\n",
    "        # Sort by relevance score\n",
    "        filtered_papers.sort(key=lambda x: x['relevance_score'], reverse=True)\n",
    "\n",
    "        return filtered_papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1be0f6",
   "metadata": {},
   "source": [
    "## <a id='toc1_6_'></a>[AdvancedQueryTranslator](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88045fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedQueryTranslator(dspy.Module):\n",
    "    \"\"\"Enhanced translator that handles complex boolean logic with Refine validation\"\"\"\n",
    "\n",
    "    def __init__(self, available_labels: List[str], label_counts: Dict[str, int], max_retries: int = 3):\n",
    "        super().__init__()\n",
    "        self.basic_translator = QueryToLabelsTranslator(available_labels, label_counts, max_retries)\n",
    "        self.available_labels = available_labels\n",
    "        self.label_counts = label_counts\n",
    "        self.max_retries = max_retries\n",
    "\n",
    "    def forward(self, query: str, force_basic: bool = False) -> BooleanQuery:\n",
    "        \"\"\"Handle complex queries with explicit boolean logic\"\"\"\n",
    "\n",
    "        # Check for explicit boolean operators\n",
    "        if not force_basic and self._has_explicit_boolean_logic(query):\n",
    "            return self._parse_explicit_boolean_query(query)\n",
    "        else:\n",
    "            return self.basic_translator(query)\n",
    "\n",
    "    def _has_explicit_boolean_logic(self, query: str) -> bool:\n",
    "        \"\"\"Check if query contains explicit AND, OR, NOT operators\"\"\"\n",
    "        boolean_keywords = ['AND', 'OR', 'NOT', 'but not', 'except', 'without']\n",
    "        return any(keyword.lower() in query.lower() for keyword in boolean_keywords)\n",
    "\n",
    "    def _parse_explicit_boolean_query(self, query: str) -> BooleanQuery:\n",
    "        \"\"\"Parse queries with explicit boolean logic\"\"\"\n",
    "\n",
    "        # This is a simplified parser - you could make it much more sophisticated\n",
    "        query_lower = query.lower()\n",
    "\n",
    "        # Split on boolean operators\n",
    "        parts = re.split(\n",
    "            r'\\b(and|or|not|but not|except|without)\\b', query_lower)\n",
    "\n",
    "        boolean_query = BooleanQuery(set(), set(), set())\n",
    "        current_mode = 'should'  # Default mode\n",
    "\n",
    "        for part in parts:\n",
    "            part = part.strip()\n",
    "            if not part:\n",
    "                continue\n",
    "\n",
    "            if part in ['and']:\n",
    "                current_mode = 'must'\n",
    "            elif part in ['or']:\n",
    "                current_mode = 'should'\n",
    "            elif part in ['not', 'but not', 'except', 'without']:\n",
    "                current_mode = 'not'\n",
    "            else:\n",
    "                # Extract labels from this part\n",
    "                part_labels = self._extract_labels_from_text(part)\n",
    "\n",
    "                if current_mode == 'must':\n",
    "                    boolean_query.must_have.update(part_labels)\n",
    "                elif current_mode == 'should':\n",
    "                    boolean_query.should_have.update(part_labels)\n",
    "                elif current_mode == 'not':\n",
    "                    boolean_query.must_not_have.update(part_labels)\n",
    "\n",
    "        return boolean_query\n",
    "\n",
    "    def _extract_labels_from_text(self, text: str) -> Set[str]:\n",
    "        \"\"\"Extract labels from a piece of text\"\"\"\n",
    "        # Use the basic translator on just this piece\n",
    "        sub_query = self.basic_translator(text)\n",
    "        return sub_query.should_have.union(sub_query.must_have)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245e8e6b",
   "metadata": {},
   "source": [
    "## <a id='toc1_7_'></a>[Testing the AdvancedQueryTranslator](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "168234be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Acelot Library data...\n",
      "Loaded 1613 papers from Acelot Library\n",
      "\n",
      "Extracting available labels from Tags column...\n",
      "Found 420 unique labels\n",
      "Sample labels with counts:\n",
      "  Machine Learning In Chemistry: 441 papers\n",
      "  In Silico Drug Discovery: 325 papers\n",
      "  Drug Discovery: 258 papers\n",
      "  Gain of Toxicity and Loss of Function: 251 papers\n",
      "  Animal Model: 236 papers\n",
      "  Reaction and Retrosynthesis Prediction: 228 papers\n",
      "  Small Molecule Therapies: 225 papers\n",
      "  ALS: 221 papers\n",
      "  Protein Ligand Binding: 208 papers\n",
      "  Misfolded Proteins: 205 papers\n",
      "\n",
      "Creating papers database...\n",
      "Created papers database with 1613 papers\n",
      "\n",
      "Initializing translator with Refine validation and label count priors...\n",
      "\n",
      "Label distribution insights:\n",
      "Most common labels: {'Machine Learning In Chemistry': 441, 'In Silico Drug Discovery': 325, 'Drug Discovery': 258, 'Gain of Toxicity and Loss of Function': 251, 'Animal Model': 236}\n",
      "Least common labels: {'AME': 1, '@shruti': 1, 'In Vitro Assay': 1, 'Irritable Bowel Disease': 1, 'autoimmune diseases': 1}\n",
      "Total label instances: 9833\n",
      "Average labels per paper: 6.10\n",
      "\n",
      "============================================================\n",
      "Query2Label with DSPy Refine + Label Count Priors - Acelot Library Test\n",
      "============================================================\n",
      "\n",
      "Query: Find papers on the discovery of new alzheimer's drugs\n",
      "Boolean Query: MUST: [\"Alzheimer's Disease\"] | SHOULD: [\"Alzheimer's Disease\", 'In Silico Drug Discovery', 'Drug Discovery']\n",
      "Found 167 papers\n",
      "  1. TDP43 nuclear export and neurodegeneration in models of amyotrophic lateral scle...\n",
      "     Score: 1.00 | Labels: ['ALS', \"Alzheimer's Disease\", 'Animal Model']\n",
      "     URL: https://newapp.readcube.com/library/c547d19c-155f-4dfa-9bfe-d6bce232ab56/item/f4187443-425f-4d88-96ea-d88e7d4e36d4\n",
      "\n",
      "  2. AÎ² oligomer concentration in mouse and human brain and its drug-induced reductio...\n",
      "     Score: 1.00 | Labels: [\"Alzheimer's Disease\", 'Amyloid Beta', 'Animal Model']\n",
      "     URL: https://newapp.readcube.com/library/c547d19c-155f-4dfa-9bfe-d6bce232ab56/item/1a1d6a60-d77e-4fc2-9292-23cb20a707c6\n",
      "\n",
      "  3. NfL makes regulatory debut as neurodegenerative disease biomarker\n",
      "     Score: 1.00 | Labels: ['ALS', \"Alzheimer's Disease\", 'Animal Model']\n",
      "     URL: https://newapp.readcube.com/library/c547d19c-155f-4dfa-9bfe-d6bce232ab56/item/21b54608-485b-4833-bed9-76810ba5ceea\n",
      "\n",
      "  4. Small molecules disaggregate alpha-synuclein and prevent seeding from patient br...\n",
      "     Score: 1.00 | Labels: ['alpha-synuclein', 'ALS', \"Alzheimer's Disease\"]\n",
      "     URL: https://newapp.readcube.com/library/c547d19c-155f-4dfa-9bfe-d6bce232ab56/item/f526047a-0e39-4450-bf62-a4e5620308e9\n",
      "\n",
      "  5. Dual Inhibitors of Amyloidâ€‘Î² and Tau Aggregation with Amyloidâ€‘Î² Disaggregating P...\n",
      "     Score: 1.00 | Labels: [\"Alzheimer's Disease\", 'Amyloid Beta', 'Amyloid Fiber']\n",
      "     URL: https://newapp.readcube.com/library/c547d19c-155f-4dfa-9bfe-d6bce232ab56/item/07436742-58f4-4528-9c81-d7aad8061362\n",
      "\n",
      "  ... and 162 more papers\n",
      "----------------------------------------\n",
      "\n",
      "Total papers in database: 1613\n",
      "Total available labels: 420\n",
      "Top 10 most used labels: ['Machine Learning In Chemistry', 'In Silico Drug Discovery', 'Drug Discovery', 'Gain of Toxicity and Loss of Function', 'Animal Model', 'Reaction and Retrosynthesis Prediction', 'Small Molecule Therapies', 'ALS', 'Protein Ligand Binding', 'Misfolded Proteins']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "# ðŸ”‘ LLM backend: point DSPy to your provider (OpenAI shown here)\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"Please set the OPENAI_API_KEY environment variable.\")\n",
    "turbo = dspy.LM(model=\"gpt-4o-mini\", temperature=0, max_tokens=16384, api_key=OPENAI_API_KEY)\n",
    "dspy.settings.configure(lm=turbo)\n",
    "\n",
    "\n",
    "# Load the Acelot Library CSV\n",
    "print(\"Loading Acelot Library data...\")\n",
    "df = pd.read_csv(\"Acelot Library.csv\")\n",
    "print(f\"Loaded {len(df)} papers from Acelot Library\")\n",
    "\n",
    "# Extract all unique tags from the Tags column to build available_labels\n",
    "print(\"\\nExtracting available labels from Tags column...\")\n",
    "all_tags = []\n",
    "for idx, row in df.iterrows():\n",
    "    tags = row.get('Tags', '')\n",
    "    if pd.notna(tags) and tags.strip():\n",
    "        # Split tags by common delimiters (semicolon, comma, pipe)\n",
    "        tag_list = []\n",
    "        for delimiter in [';', ',', '|']:\n",
    "            if delimiter in tags:\n",
    "                tag_list = [tag.strip() for tag in tags.split(delimiter)]\n",
    "                break\n",
    "        else:\n",
    "            # No delimiter found, treat as single tag\n",
    "            tag_list = [tags.strip()]\n",
    "        \n",
    "        # Add all tags to the list (for counting)\n",
    "        all_tags.extend([tag for tag in tag_list if tag])\n",
    "\n",
    "# Count label occurrences\n",
    "label_counts = Counter(all_tags)\n",
    "available_labels = list(label_counts.keys())\n",
    "\n",
    "print(f\"Found {len(available_labels)} unique labels\")\n",
    "print(\"Sample labels with counts:\")\n",
    "for label, count in list(label_counts.most_common(10)):\n",
    "    print(f\"  {label}: {count} papers\")\n",
    "\n",
    "# Create papers_db from DataFrame\n",
    "print(\"\\nCreating papers database...\")\n",
    "papers_db = []\n",
    "for idx, row in df.iterrows():\n",
    "    # Extract tags for this paper\n",
    "    tags = row.get('Tags', '')\n",
    "    paper_labels = []\n",
    "    if pd.notna(tags) and tags.strip():\n",
    "        # Split tags by common delimiters\n",
    "        for delimiter in [';', ',', '|']:\n",
    "            if delimiter in tags:\n",
    "                paper_labels = [tag.strip() for tag in tags.split(delimiter)]\n",
    "                break\n",
    "        else:\n",
    "            paper_labels = [tags.strip()]\n",
    "        \n",
    "        # Clean up labels\n",
    "        paper_labels = [tag for tag in paper_labels if tag]\n",
    "    \n",
    "    paper = {\n",
    "        'id': idx,\n",
    "        'title': row.get('Title', 'No Title'),\n",
    "        'abstract': row.get('Abstract', 'No Abstract'),\n",
    "        'labels': paper_labels,\n",
    "        'library_url': row.get('Library URL', ''),\n",
    "        'year': row.get('year', ''),\n",
    "        'journal': row.get('Journal', ''),\n",
    "        'authors': row.get('Author', '')\n",
    "    }\n",
    "    papers_db.append(paper)\n",
    "\n",
    "print(f\"Created papers database with {len(papers_db)} papers\")\n",
    "\n",
    "# Initialize translator and filter with Refine validation and label counts\n",
    "print(\"\\nInitializing translator with Refine validation and label count priors...\")\n",
    "\n",
    "# Create translator using Refine pattern with label counts\n",
    "translator = AdvancedQueryTranslator(available_labels, label_counts, max_retries=3)\n",
    "paper_filter = PaperFilter(papers_db)\n",
    "\n",
    "# Show label distribution insights\n",
    "print(f\"\\nLabel distribution insights:\")\n",
    "print(f\"Most common labels: {dict(label_counts.most_common(5))}\")\n",
    "print(f\"Least common labels: {dict(label_counts.most_common()[-5:])}\")\n",
    "print(f\"Total label instances: {sum(label_counts.values())}\")\n",
    "print(f\"Average labels per paper: {sum(label_counts.values()) / len(papers_db):.2f}\")\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"Find papers on the discovery of new alzheimer's drugs\",\n",
    "    # \"Find papers on protein folding using machine learning\",\n",
    "    # \"Drug discovery with graph neural networks\",\n",
    "    # \"Machine learning but not computational chemistry\",\n",
    "    # \"Deep learning AND molecular dynamics\",\n",
    "    # \"Transformers OR neural networks for chemistry\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Query2Label with DSPy Refine + Label Count Priors - Acelot Library Test\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    try:\n",
    "        boolean_query = translator(query, force_basic=True)\n",
    "        print(f\"Boolean Query: {boolean_query}\")\n",
    "\n",
    "        filtered_papers = paper_filter.filter_papers(boolean_query)\n",
    "        print(f\"Found {len(filtered_papers)} papers\")\n",
    "        \n",
    "        # Show top 5 results\n",
    "        for i, paper in enumerate(filtered_papers[:5]):\n",
    "            print(f\"  {i+1}. {paper['title'][:80]}...\" if len(paper['title']) > 80 else f\"  {i+1}. {paper['title']}\")\n",
    "            print(f\"     Score: {paper.get('relevance_score', 0):.2f} | Labels: {paper['labels'][:3]}\")\n",
    "            if paper.get('library_url'):\n",
    "                print(f\"     URL: {paper['library_url']}\")\n",
    "            print()\n",
    "        \n",
    "        if len(filtered_papers) > 5:\n",
    "            print(f\"  ... and {len(filtered_papers) - 5} more papers\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing query: {e}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(f\"\\nTotal papers in database: {len(papers_db)}\")\n",
    "print(f\"Total available labels: {len(available_labels)}\")\n",
    "print(\"Top 10 most used labels:\", [label for label, _ in label_counts.most_common(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b6fa552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-07-05T18:49:47.119022]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `query` (str): Natural language query from user\n",
      "2. `available_labels` (str): List of available labels in the database which will be used to match concepts in the query to actual labels (comma-separated)\n",
      "Your output fields are:\n",
      "1. `reasoning` (str): \n",
      "2. `main_concepts` (str): Primary concepts the user is looking for (comma-separated)\n",
      "3. `required_concepts` (str): Concepts that MUST be present (comma-separated, empty if none)\n",
      "4. `optional_concepts` (str): Concepts that SHOULD be present but not required (comma-separated, empty if none)\n",
      "5. `excluded_concepts` (str): Concepts that must NOT be present (comma-separated, empty if none)\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## query ## ]]\n",
      "{query}\n",
      "\n",
      "[[ ## available_labels ## ]]\n",
      "{available_labels}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## main_concepts ## ]]\n",
      "{main_concepts}\n",
      "\n",
      "[[ ## required_concepts ## ]]\n",
      "{required_concepts}\n",
      "\n",
      "[[ ## optional_concepts ## ]]\n",
      "{optional_concepts}\n",
      "\n",
      "[[ ## excluded_concepts ## ]]\n",
      "{excluded_concepts}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Parse natural language query into structured components using the available labels. Think broadly about the different clusters of labels.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## query ## ]]\n",
      "Find papers on the discovery of new alzheimer's drugs\n",
      "\n",
      "[[ ## available_labels ## ]]\n",
      "Machine Learning In Chemistry (count=441), In Silico Drug Discovery (count=325), Drug Discovery (count=258), Gain of Toxicity and Loss of Function (count=251), Animal Model (count=236), Reaction and Retrosynthesis Prediction (count=228), Small Molecule Therapies (count=225), ALS (count=221), Protein Ligand Binding (count=208), Misfolded Proteins (count=205), TDP-43 (count=202), Template-Based Reaction Prediction (count=200), Stress Granules (count=189), Graph Neural Networks for Chemistry (count=174), Alzheimer's Disease (count=167), Protein Structure Function (count=163), Generative Models (count=161), Molecular Representations (count=157), Important Datasets (count=150), Molecular Dynamics (count=143), Intrinsically Disordered Proteins (count=139), Amyloid Fiber (count=131), High-throughput Experimentation (count=127), Cheminformatics (count=125), Frontotemporal Dementia (count=122), Protein Disaggregators (count=120), Thioflavin T Assay (count=105), C9Orf72 (count=103), Parkinson's Disease (count=102), Docking Calculations (count=96), Protein Protein Interactions (count=95), Potential Targets (count=87), De Novo Drug Design (count=83), Amyloid Beta (count=80), Transformers in Chemistry (count=79), Reaction Mechanism (count=69), Kinase Inhibitor (count=67), ADME (count=67), RNA-Seq (count=66), Genome Wide Association Study (count=65), HIT Discovery (count=65), Large Language Models (count=63), RNA (count=63), Protein Folding (count=63), New Target Discovery (count=60), Kinases (count=59), Clinical Trial (count=56), CNS Drugs (count=53), Quantitative Structure Activity Relationship (count=52), Graph Neural Networks in Computational Biology (count=52), Phase Separation (count=47), alpha-synuclein (count=47), Analysis of Protein Conformational Dynamics (count=47), Fluorescence Microscopy (count=47), Representation Learning (count=46), Hit-to-Lead (count=46), Neurons (count=45), Fragment Based Drug Design (count=45), iPSC (count=44), Signaling Pathways (count=43), Cryo-EM (count=43), Automated Synthesis Platform (count=43), Multi-Task Learning (count=43), Elementary Reactions (count=43), Fine-Tuning (count=41), TMEM106B (count=41), Immunoblotting (count=40), Post-Translational Modifications (count=40), Crystal Structures (count=38), Targeting RNA (count=38), Explainability in AI (count=38), Reinforcement Learning in Chemistry (count=33), Transition States (count=33), Predicting Toxicity (count=30), MAPK Kinase (count=29), Cancer (count=29), Neurodegernative Diseases (count=28), Nemo-Like Kinases (count=27), Prompt Engineering (count=27), Lysosome (count=27), Mitochondrial Dysfunction (count=27), Western Blot (count=25), In Context Learning (count=22), Chemical Similarity Searching (count=22), Atomic Force Microscopy (count=22), Phosphorylation (count=22), Coarse Graining (count=21), DMPK (count=21), Active Learning in Chemistry (count=21), Variational Autoencoder (count=20), Drug Repurposing (count=20), Plasma Proteins (count=19), Inflammation (count=18), Diffusion Models (count=18), Protein Stabilizers (count=18), Single Nucleotide Polymorphism (count=18), Shape Based Methods in Comp Chem (count=17), Allosteric Pockets (count=17), IAPP (count=17), Lipocalin-2 (count=17), Genomics (count=17), Corrector Drugs (count=17), AlphaFold (count=16), Synthetic Complexity Score (count=16), Growth Factors (count=16), Graph Neural Network (count=16), TMEM175 (count=16), Predicting Rates of Reactions (count=15), Equivariant Neural Network (count=15), G-Protein Coupled Receptor (count=15), Agonists and Antagonists (count=15), Transmission Electron Microscopy (count=15), Tankyrase (count=15), Cyclooxygenase Enzymes (count=15), Self-Supervised Machine Learning (count=14), Transcription Factors (count=14), Ubiquitin (count=14), Dipeptide Repeat Proteins (count=14), Biomarker (count=14), Progranulin (count=14), cGAS-STING pathway (count=14), Blood Brain Barrier (BBB) (count=14), Cross-Coupling Reactions (count=13), Few Shot Learning (count=13), P38 Kinase (count=13), UNC13A (count=13), Lead Generation and Optimization (count=13), Pharmokinetics (count=13), Proteomics (count=13), ELISA (count=12), SOD1 (count=12), Medicinal Chemistry (count=12), Catalysis (count=12), Prions (count=12), AMES (count=12), Natural Language Understanding (count=12), Monte Carlo Tree Search (count=12), Ion Channels (count=12), GFP (count=11), PGC-1a (count=11), Retrieval Augmented Generation (count=11), RNA Foci (count=11), Contextual Representation (count=11), Metabolic Clearance (count=11), Graph Transformers (count=11), Contrastive Learning (count=11), Mitochondria (count=11), Infrared Spectroscopy (count=10), Late Stage Functionalization (count=10), Covalent Inhibitors (count=10), Quantitative Estimate of Druglikeness (count=9), Transcription (Biology) (count=9), Proteosome (count=9), Knowledge Graph (count=9), Autophagy (count=9), Circular Dichroism (count=9), Computational Chemistry (count=9), Kp-uu-bloodbrain parameter (count=9), Phenotypic Screening (count=9), Elk-1 (count=9), Huntingtin Protein (count=8), Astrocytes (count=8), book (count=8), Ensemble Learning (count=8), Diabetes (count=8), Foundational Model (count=8), Metabolism (count=8), Biogenesis (count=8), T5 (count=8), Recurrent Neural Network (count=8), LLM Agents (count=8), Bayesian Optimization (count=8), Graph Attention Networks (count=7), Generative Adversarial Network (count=7), Parylation (count=7), BERT (count=7), Bioisostere (count=7), LogS (count=7), Genetic Algorithm (count=7), Uncertainty Quantification (count=7), cGAS (count=7), Reaction Fingerprint (count=7), Transfer Learning in Chemistry (count=7), Biomedicine (count=7), Hypothesis about Parallel vs Anti-Parallel Conformation (count=6), mTOR Inhibitors (count=6), Iron (count=6), Free Energy Perturbation Methods (count=6), Lipophilicity (count=6), Aldehyde Oxidase (count=6), Multi-label classification (count=6), GraphRAG (count=6), Transfer Learning (count=6), p53 Protein (count=6), Antisense Oligonucleotides (ASOs) (count=6), SIRT1 (count=6), Tau Protein (count=6), PROTAC (count=6), Lysosomal Dysfunction (count=6), Huntington's Disease (count=5), SMILES (count=5), Point Cloud Representation (count=5), Normalizing Flows (count=5), Antibody (count=5), hERG (count=5), Scaffold Hopping (count=5), Amyloid Structure Prediction (count=5), Named Entity Recognition (count=5), Enzymes (count=5), Graphs (count=5), GNN on Point Clouds (count=5), 5-HT2A (count=5), CNS MPO Scores (count=5), Tox21 (count=5), Reinforcement Learning (count=5), Enzyme Inhibitors (count=5), Protein Ligand Interaction (count=5), Protein Engineering (count=4), Multi-modal Machine Learning (count=4), Instruction Fine-Tuning (count=4), Wnt Signaling Pathway (count=4), Fused in Sarcoma (FUS) (count=4), Information Retrieval (count=4), Replica Exchange (count=4), Dopaminergic (count=4), Type I Interferons (count=4), Graph representation Learning (count=4), Convolutional Neural Network (count=4), Zero Shot Learning (count=4), Unsupervised Learning (count=4), Piperazine (count=4), CLint (count=4), PCR (count=4), SUMOylation (count=4), Nuclear Magnetic Resonance Spectroscopy (count=3), Patents (count=3), Dopamine (count=3), Cryptic Splicing (count=3), SH-SY5Y (count=3), CRISPR (count=3), Demonstrate Search Predict (count=3), Acelot (count=3), GCase (count=3), APOE4 Gene (count=3), GTPase (count=3), Binding Affinity (count=3), Nuclear Localization (count=3), Graph Laplacian (count=3), Antipsychotics (count=3), Mitophagy (count=3), Arthritis (count=3), HYDE (count=3), t-distributed stochastic neighbor embedding (count=2), Gromacs (count=2), Hypothesis about NLK Homodimerization (count=2), Genetic Algorithms for De Novo Design (count=2), AKT (count=2), mTOR Kinase (count=2), Conditions (count=2), Random Forests (count=2), Astrogliosis (count=2), Actor Critic Methods (count=2), Maximum Common Substructure (count=2), Apoptosis (count=2), RNAi (count=2), eIF2B Protein (count=2), Transthyretin Amyloidoses (count=2), Principle Component Analysis (count=2), Restricted Boltzmann Machines (count=2), 3D-QSAR (count=2), Heart Disease (count=2), Efflux Ratio (count=2), Nicotinamide Adenine Dinucleotide (count=2), Autodock (count=2), Stathmin-2 (count=2), Acidity (count=2), Computational Chemistry Software (count=2), Mass Spectroscopy (count=2), Graph Isomorphism Neural Network (count=2), Transmembrane Proteins (count=2), Graph RNN (count=2), Imitation Learning (count=2), Graph Classification with Graph Neural Networks (count=2), Semantic Scholar (count=2), ChEMBL (count=2), Machine Learning (count=2), AMP-activated protein kinase (AMPK) (count=2), Dynamic Light Scattering (count=2), Nuclear Pore Complex (count=2), Photoaffinity Labeling (count=2), Journal Club (count=2), Antibiotics (count=2), Question Answering (QA) Tasks (count=2), Chain-of-Thought Prompting (count=2), Flexible Docking (count=2), TDP-43 Structure (count=2), DEREK (count=2), DNA (count=2), GPT-3 (count=1), Cytokines (count=1), Dopamine D2 (count=1), Gradient Boosted Trees (count=1), Mol2Vec (count=1), Probability Calibration (count=1), ROC-AUC (count=1), Toxicology (count=1), Predicting Molecule-Molecule-Interactions (count=1), Adeno-Associated Virus (AAV) (count=1), Morgan Fingerprint (count=1), Segmentation in Machine Learning (count=1), Annexin A11 (count=1), Flare-A Cresset CADD Software (count=1), Pareto Optimization (count=1), Turbidity Measurement (count=1), Bioinformatics (count=1), Reaction Discovery (count=1), Nucleopore (count=1), KNIME (count=1), Naive Bayes (count=1), QikProp (count=1), Dot Blot Technique (count=1), Heterogeneous graph learning (count=1), Heterogeneous Network Edge Prediction.pdf#page=1&selection=35 (count=1), UMAP (count=1), Variational Graph Autoencoder (count=1), Drug Drug Interaction (count=1), Nonsense Mediated Decay (count=1), Clustering (count=1), DBSCAN (count=1), Dimensionality Reduction (count=1), K-means (count=1), Time-Lagged Independent Component Analysis (count=1), Golgi (count=1), SMOTE (count=1), Voronoi Tessellation (count=1), Multi-Armed Bandits (count=1), NF-ÎºB Signaling Pathway (count=1), Transition State Theory (count=1), Schrodinger (count=1), X-Ray Diffraction (count=1), Riluzole and Edarvone (count=1), Sanofi (count=1), STMN2 (count=1), Heterogenous Graph Attention Network (count=1), Lipocalin (count=1), Protease (count=1), Retrieval Augmented Classification (count=1), Q-Learning (count=1), Analysis of Aggregate Ligand Interaction (count=1), Interferon Inducible Cells (count=1), Malaria (count=1), Lipids (count=1), LIME (count=1), Molecular Machines (count=1), Cystic Fibrosis (count=1), Exploration and Exploitation (count=1), Self Organizing Maps (count=1), Epitope (count=1), Two-Dimensional Infrared Spectroscopy (count=1), Atomwise (count=1), mRNA (count=1), Glioblastoma (count=1), GPU (count=1), Convex Optimization (count=1), Deep Belief Network (count=1), Multiple Sclerosis (count=1), A2A Target (count=1), Preladenant (count=1), Domain Applicability (count=1), Ligand Efficiency (count=1), Int (count=1), Next Generation Sequencer (count=1), Deep Reinforcement Learning (count=1), Vector Embedding (count=1), Microglia (count=1), A2A Receptor (count=1), DNA Encoded Libraries (count=1), Over Expression (count=1), SentencePiece Tokenizer (count=1), Thermochemistry (count=1), Classification and Regression Trees (count=1), Neural Information Retrieval (count=1), CYP P450 (count=1), Baeyer-Villiger Reaction (count=1), Lysergic Acid Diethylamide (count=1), Meta Learning (count=1), Message Passing Network (count=1), Graph Autoencoder (count=1), Proteins (count=1), HEK Cells (count=1), Nuclear Overhauser Effect Spectroscopy (NOESY) (count=1), Active Learning (count=1), Induced Fit Docking (count=1), Metadynamics (count=1), Tanimoto Similarity (count=1), Link Prediction in Graphs (count=1), Generative Active Learning (count=1), Text Attributed Graphs (count=1), Cathepsin B (count=1), Llama (count=1), Learned Position Embeddings (count=1), Text-Attributed Graphs (count=1), Energy Based Models (count=1), Markov Process (count=1), Ideas for using LLMs in Obsidian (count=1), LongFormQA (count=1), Alpaca (count=1), inflammation (count=1), Graph Neural Networksfor Chemistry (count=1), pKd prediction (count=1), delta NLS mouse (count=1), Gain of Toxicity andÂ Loss of Function (count=1), Graph Neural NetworksÂ in ComputationalÂ Biology (count=1), LRRK2 (count=1), Structural Alerts (count=1), AME (count=1), @shruti (count=1), In Vitro Assay (count=1), Irritable Bowel Disease (count=1), autoimmune diseases (count=1)\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## main_concepts ## ]]`, then `[[ ## required_concepts ## ]]`, then `[[ ## optional_concepts ## ]]`, then `[[ ## excluded_concepts ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## reasoning ## ]]\n",
      "The query is focused on finding research papers related to the discovery of new drugs for Alzheimer's disease. The main concept here is Alzheimer's Disease, which is directly mentioned in the query. Additionally, the process of drug discovery is relevant, so the labels related to drug discovery, such as Drug Discovery and In Silico Drug Discovery, are also pertinent. The user is likely looking for papers that discuss new therapeutic approaches or compounds targeting Alzheimer's, hence the emphasis on drug discovery methodologies.\n",
      "\n",
      "[[ ## main_concepts ## ]]\n",
      "Alzheimer's Disease, Drug Discovery, In Silico Drug Discovery\n",
      "\n",
      "[[ ## required_concepts ## ]]\n",
      "Alzheimer's Disease\n",
      "\n",
      "[[ ## optional_concepts ## ]]\n",
      "Drug Discovery, In Silico Drug Discovery\n",
      "\n",
      "[[ ## excluded_concepts ## ]]\n",
      " \n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "turbo.inspect_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f605b0f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "myenv (3.10.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
